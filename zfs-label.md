## ZFS Label

В данном разделе описаны особенности меток ZFS, и свзязанных с ними процессов.

Метки ZFS присутствуют на физическом носителе, если на него производится запись данных (обычные диски в raidz, mirros и slog).
Для повышения надёжности на диске присутствуют 4 метки - две в начале диска и две в конце (по номерам, соответственно, 1, 2 и 3, 4).
Кроме того, при обновлении данных в метках вначале обновляются первая и третья, а потом - вторая и четвёртая.
Это также повышает отказоустойчивость: если произошла ошибка при записи одной из пар, то может использоваться другая пара.

Рассмотрим структуры меток (рис. @fig:zfs-label). В каждой метке присутствуют NVList с описанием конфигурации пула и массив уберблоков.
Перед первой парой меток дополнительно отступается свободное пространство (8k) под EFI-метки и место под `Boot Block Header` (8k).
В текущей реализации `Boot Block Header` просто заполняется нулями с контрольной суммой от этих нулей при создании меток.


![Структура метки ZFS](./img/zfs-label.png "Структура метки ZFS"){#fig:zfs-label}

### Виртуальные устройства (VDEV) и метки на них


| Тип vdev | Описание                                          | Тип узла      | Тип метки     |
| -------- | ------------------------------------------------- | ------------- | ------------- |
| disk     | физическое устройство, на котором хранятся данные | leaf          | self          |
| file     | файл, на котором хранятся данные                  | leaf          | self          |
| log      | кэш на запись                                     | leaf/internal | property      |
| cache    | кэш на чтение                                     | aux           | aux           |
| mirror   | RAID-массив                                       | internal      | self/children |
| raidz    | RAID-массив                                       | internal      | self/children |
| missing  | устройство, не найденное во время импорта         | leaf          | None          |
| root     | корневой vdev пула                                | root          | all           |
| spare    | устройство для горячей замены                     | aux           | aux           |

Table: Типы виртуальных устройств пула {#tbl:vdev-types}

В табл. {@tbl:vdev-types} представлено описание виртуальных устройств, из которых может состоять пул. 
Если исходить из практических соображений (возможностей команды `zpool`), то дерево vdev'ов в предельном случае может состоять из:

* корень `root` (root vdev);

* промежуточные узлы `internal` (mirror, raidz);

* листовые узлы `leaf` (disk, file);

* `aux`-устройства (spare, cache);

Причём, промежуточные узлы могут располагаться на нескольких уровнях. 
К примеру, программа `ztest` при проведении тестирования может создавать `"RAID610"` - страйп из зеркал, каждое из которых
состоит из нескольких `raidz1`.
Однако, в на практике, не существует способа создания таких "ветвистых" пулов. Поэтому примем следующую терминологию
(и в дальнейшем будем ей пользоваться):

* leaf - листовые узлы (disk, file);

* top-level vdev - промежуточные узлы (mirror, raidz);

* root - корневой vdev пула;

* aux - устройства, не хранящие данные (cache и spare);

* log - листовой или промежуточный узел, у которого выставлен флаг `is_log`;

Рассмотрим рис. @fig:zfs-raid10-raid1log. В нём представлен RAID10 пул с RAID1-кэшем на запись.
Выделенная штриховкой область - vdev'ы, состояние которых будет отражено в метках устройств `sdq1` и `sdq2`.
Желтым выделен vdev, у которого установлен флаг `is_log`. Если бы лог-устройство было бы не зеркалом, а обычными
дисками или страйпами, то соответсвующий флаг был бы установлен у них.

![Гибридный пул RAID10 + log(RAID1)](./img/zfs-raid10-raid1log.png "Гибридный пул RAID10 + log(RAID1)"){#fig:zfs-raid10-raid1log}

Таким образом, в метках, хранящихся на дисках/файлах отражено состояние данного "листового" vdev'a, его "братьев" и родителя.
А также индекс родителя в массиве `root.vdev_tree` (где root - корневой vdev пула).

Пусть мы создаём следующий пул (см. рис @fig:zfs-raid10-raid1log):

\newpage

```bash
zpool create -f tank \
	mirror /dev/sdq1 /dev/sdq2 \
	mirror /dev/sdq3 /dev/sdq4 \
	log mirror /dev/sdq5 /dev/sdq6 \
	cache /dev/sdq7 /dev/sdq8 \
	spare /dev/sdq9 
zpool status
#  pool: tank
# state: ONLINE
#  scan: none requested
#config:
#
#        NAME        STATE     READ WRITE CKSUM
#        tank        ONLINE       0     0     0
#          mirror-0  ONLINE       0     0     0
#            sdq1    ONLINE       0     0     0
#            sdq2    ONLINE       0     0     0
#          mirror-1  ONLINE       0     0     0
#            sdq3    ONLINE       0     0     0
#            sdq4    ONLINE       0     0     0
#        logs
#          mirror-2  ONLINE       0     0     0
#            sdq5    ONLINE       0     0     0
#            sdq6    ONLINE       0     0     0
#        cache
#          sdq7      ONLINE       0     0     0
#          sdq8      ONLINE       0     0     0
#        spares
#          sdq9      AVAIL   
#
#errors: No known data errors
```

Информация о структуре пула будет отражена только в  метках дисков `sdq[1-6]`. О дисках `sdq[7-9]` на этих метках не будет никакой
информации, т.к. эти диски относятся к типу `aux` `vdev`. `aux`-устройства хранят в своих метках только 3 составляющие: версия пула, тип устройства и guid устройства.
Каким же образом ZFS узнаёт, где их искать при импорте? Ответ на этот вопрос - в метаданных пула. Именно там и хранятся пути к этим устройствам.
Таким образом, при импорте, на этапе анализа меток aux-устройства не участвуют в "сборе" дерева пула. Это происходит уже после получения `uberblock`'a и чтения метаданных (MOS).
В дополнение к этому, ZFS спокойно относится к созданию пулов в которых присутствуют устройства с `aux`-метками (нет необходимости в ключе `-f` для команды `zpool`).
Более того, при импорте на этих устройствах затираются данные - ZFS пишет туда свои метки и ей безразлично, хранится ли там информация.


### Конфигурация пула (ZFS Label NVList)

В NVList хранится информация о пуле и части соседних с текущим vdev'ов.
Как было раннее указано, хранится информация только о важных (с точки зрения целостности данных) vdev'ах - диски с данными и slog.
Другие виды vdev'ов (l2cache, spare) хранятся вне меток (непосредственно внутри пула).
"Соседние" vdev'ы - те устройства (виртуальные и реальные) с которыми граничит данное устройство в дереве vdev'ов пула.
Именно о них и хранится информация в метке диска/файла.
Одна из составляющих метки - список конфигурационных параметров пула.
NVList конфигурации пула включает в себя следующие элементы:

* версия OnDisk-формата;

* имя пула;

* состояние пула (активный, экспортированный, уничтоженный);

* номер транзакционной группы, в которой была записана метка;

* guid пула;

* guid vdev'а верхнего уровня;

* дерево соседних vdev'ов;

Дерево vdev'ов содержит элементы - массив, состоящий из элементов:

* type: имя типа (диск, файл, зеркало и т.п.);

* id: уникальный индекс vdev'а в родительском vdev'е;

* guid vdev'a;

* path: путь к устройству (только для "листовых" vdev'ов - диски, файлы)

* metaslab_array: номер объекта (внутри пула), который содержит массив номеров объектов, каждый из которых соответствует объекту, описывающему metaslab;

* metaslab_shift: log2 от размера метаслаба;

* ashift (allocatable shift): log2 от размера минимальной области, которая может быть выделена в top-level vdev'е. Например, если ashift=9, то в данном vdev'е области будут выделяться не меньшие, чем 512 байт;

* asize (allocatable size): количество байт, которое можно выделить в данном vdev'е. Грубо говоря, общий объем всех дочерних vdev'ов;

* children: список дочерних vdev'ов;


### Уберблоки (Uberblocks)

Следующий элемент метки - уберблоки (Uberblocks). В текущей реализации используется массив из 128 уберблоков. Размер уберблока - 1k.
ZFS работает с уберблоками как с кольцевым буффером: при обновлении перезаписывается самый старый уберблок.
Таким образом, теоретически, мы в любой момент времени имеем до 128 readonly состояний файловой системы (на самом деле меньше - см. раздел [метаслабы](#метаслабы)).
При поиске уберблока (во время операции импорта) выбирается уберблок, соответствующий следующим критериям:

* верная контрольная сумма;

* наибольшее значение временной метки;

* наибольшее значение номера транзакционной группы;

В случае, если существуют проблемы при импорте пула, ZFS позволяет производить операцию отмотки (REWIND), которая соответствует опциям `-F` и `-X` команды `zpool import`.
Отмотка позволяет "откатиться" в одно из предыдущих состояний файловой системы посредством выбора более "молодого" уберблока (своего рода перемещение во времени в рамках последних 128 транзакций).
Кроме того, существует опция `-T`, которая позволяет задать конкретный номер транзакции, к которой нужно откатиться.

Уберблок состоит из следующих элементов:

* ub_magic: уникальное "магическое" число (magic number), которое позволяет не только определить, что область на диске содержит уберблок, но определить порядок байт,
определённый в пуле (ZFS пулы переносимы между системами с различными `endianness`):

	- Big-Endian: 0x00bab10c (oo-ba-block)
	- Little-Endian: 0xcb1ba00

* ub_version: версия OnDisk-формата (см. версию в NVList)

* ub_txg: номер транзакционной группы, в которой был записан данный уберблок. Должен быть не меньше, чем параметр `txg` в метке;

* ub_guid_sum: сумма guid всех листовых vdev'ов. Используется для проверки доступности всех vdev'ов пула. Если это поле не совпадает с вычисленной при импорте суммой, значит было потеряно устройство;

* ub_timestamp: временная метка на момент записи уберблока;

* ub_rootbp: blktr, указывающий на расположение MOS (Meta Object Set) данного пула;


### Просмотр меток

Для просмотра содержимого меток пула можно воспользоваться командой `zdb <pool-name>`. 
Или `zdb <pool-name> -e -p <dir>` если пул был экспортирован и файлы/диски, из которых состоит пул находятся в директории `<dir>`.



